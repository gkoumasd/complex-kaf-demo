{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "guZZKDsBB0sH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Complex-valued KAF applied on the Fashion-MNIST dataset\n",
        "\n",
        "Demo from the following paper: https://arxiv.org/abs/1802.08026"
      ]
    },
    {
      "metadata": {
        "id": "GpzxIKj8CTrr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Install prerequisites"
      ]
    },
    {
      "metadata": {
        "id": "Xg6Pg8Z2BhFr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b5a7dc1f-a883-4381-f27f-c93b591ef298",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528287632147,
          "user_tz": -120,
          "elapsed": 4226,
          "user": {
            "displayName": "Simone Scardapane",
            "photoUrl": "//lh6.googleusercontent.com/-ZK0nd0iCe7w/AAAAAAAAAAI/AAAAAAAAA_o/pNhWxdOsGTI/s50-c-k-no/photo.jpg",
            "userId": "115632300554726808471"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The code requires autograd\n",
        "!pip install autograd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autograd\n",
            "  Downloading https://files.pythonhosted.org/packages/08/7a/1ccee2a929d806ba3dbe632a196ad6a3f1423d6e261ae887e5fef2011420/autograd-1.2.tar.gz\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.3)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\n",
            "Building wheels for collected packages: autograd\n",
            "  Running setup.py bdist_wheel for autograd ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/72/6f/c2/40f130cca2c91f31d354bf72de282922479c09ce0b7853c4c5\n",
            "Successfully built autograd\n",
            "Installing collected packages: autograd\n",
            "Successfully installed autograd-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MHguJxRuCYSE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c5a80227-343e-4d43-cfa1-aa0790adb535",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528287658556,
          "user_tz": -120,
          "elapsed": 26384,
          "user": {
            "displayName": "Simone Scardapane",
            "photoUrl": "//lh6.googleusercontent.com/-ZK0nd0iCe7w/AAAAAAAAAAI/AAAAAAAAA_o/pNhWxdOsGTI/s50-c-k-no/photo.jpg",
            "userId": "115632300554726808471"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Download Fashion MNIST repository\n",
        "!git clone https://github.com/zalandoresearch/fashion-mnist.git\n",
        "  \n",
        "# Add Fashion MNIST to path\n",
        "import sys\n",
        "sys.path.insert(0,'fashion-mnist')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fashion-mnist'...\n",
            "remote: Counting objects: 615, done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 615 (delta 5), reused 9 (delta 4), pack-reused 603\u001b[K\n",
            "Receiving objects: 100% (615/615), 105.18 MiB | 22.20 MiB/s, done.\n",
            "Resolving deltas: 100% (347/347), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NkfSRq5SCd8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Data loading"
      ]
    },
    {
      "metadata": {
        "id": "LSGSj5YICfai",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "99b9c1bc-7bc9-4bd8-ffdc-cf726604d840",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528287876254,
          "user_tz": -120,
          "elapsed": 10226,
          "user": {
            "displayName": "Simone Scardapane",
            "photoUrl": "//lh6.googleusercontent.com/-ZK0nd0iCe7w/AAAAAAAAAAI/AAAAAAAAA_o/pNhWxdOsGTI/s50-c-k-no/photo.jpg",
            "userId": "115632300554726808471"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from autograd import numpy as np\n",
        "from sklearn import preprocessing, model_selection\n",
        "from numpy.fft import fft2\n",
        "import utils.mnist_reader as mnist_reader\n",
        "      \n",
        "# Load data\n",
        "X_train, y_train = mnist_reader.load_mnist('fashion-mnist/data/fashion', kind='train')\n",
        "X_test, y_test = mnist_reader.load_mnist('fashion-mnist/data/fashion', kind='t10k')\n",
        " \n",
        "# Perform one-hot encoding on the output values  \n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "y_train = onehot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test = onehot_encoder.fit_transform(y_test.reshape(-1, 1))\n",
        "        \n",
        "    \n",
        "# Compute the Fourier transform (training)\n",
        "Xfft_train = np.zeros((X_train.shape[0], X_train.shape[1])) + 1j*np.zeros((X_train.shape[0], X_train.shape[1]))\n",
        "print('\\nComputing Fourier transforms (training)... ')\n",
        "for i in range(X_train.shape[0]):\n",
        "  Xfft_train[i, :] = fft2(X_train[i, :].reshape(28, 28)).reshape(-1)\n",
        "     \n",
        "# Compute the Fourier transform (test)\n",
        "Xfft_test = np.zeros((X_test.shape[0], X_test.shape[1])) + 1j*np.zeros((X_test.shape[0], X_test.shape[1]))\n",
        "print('\\nComputing Fourier transforms (test)... ')\n",
        "for i in range(X_test.shape[0]):\n",
        "  Xfft_test[i, :] = fft2(X_test[i, :].reshape(28, 28)).reshape(-1)\n",
        "    \n",
        "# Compute most significant coefficients\n",
        "significance = np.mean(np.absolute(Xfft_train), axis=0)\n",
        "significance_idx = np.argsort(significance)\n",
        "    \n",
        "# Remove non significant coefficients and scale the features   \n",
        "scaler = preprocessing.MinMaxScaler(feature_range=(-1.0, +1.0))\n",
        "Xfft_train = scaler.fit_transform(np.real(Xfft_train[:, significance_idx[-100:]])) + 1j*scaler.fit_transform(np.imag(Xfft_train[:, significance_idx[-100:]]))\n",
        "Xfft_test = scaler.fit_transform(np.real(Xfft_test[:, significance_idx[-100:]])) + 1j*scaler.fit_transform(np.imag(Xfft_test[:, significance_idx[-100:]]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Computing Fourier transforms (training)... \n",
            "\n",
            "Computing Fourier transforms (test)... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "05ZzSUk8DdFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3 - Define KAF neural network"
      ]
    },
    {
      "metadata": {
        "id": "9wTId6xdEJsR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Auxiliary functions\n",
        "\n",
        "def softmax(s):\n",
        "    tmp = np.exp(s)\n",
        "    return tmp/np.sum(tmp, axis=1).reshape(-1, 1)\n",
        "    \n",
        "def gauss_kernel(X, D, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Compute the 1D Gaussian kernel between all elements of a \n",
        "    NxH matrix and a fixed L-dimensional dictionary, resulting in a NxHxL matrix of kernel\n",
        "    values.\n",
        "    \"\"\"\n",
        "    return np.exp(- gamma*np.square(X.reshape(-1, X.shape[1], 1) - D))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37t0WsETDXqh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "### SPLIT KAF NEURAL NETWORK ##################################################\n",
        "###############################################################################\n",
        "\n",
        "def init_split_kaf_nn(d, classes, layers=[], rs=np.random.RandomState(0), dict_size=20, boundary=2.0):\n",
        "    \"\"\" \n",
        "    Initialize the parameters of a split KAF feedforward network.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Add initial and final layer\n",
        "    l = layers.copy()\n",
        "    l.insert(0, d)\n",
        "    l.append(classes)\n",
        "    \n",
        "    # Initialize dictionary\n",
        "    D = np.linspace(-boundary, boundary, dict_size).reshape(1, 1, -1)\n",
        "    \n",
        "    # Rule of thumb for gamma\n",
        "    interval = D[0,0,0]-D[0,0,1];\n",
        "    sigma = 2*interval # empirically chosen\n",
        "    gamma = 0.5/np.square(sigma)\n",
        "\n",
        "    # Initialize parameters\n",
        "    w = [(rs.randn(insize, outsize)*0.01 + 1j*rs.randn(insize, outsize)*0.01,   # weight matrix\n",
        "                     rs.randn(outsize)*0.01 + 1j*rs.randn(outsize)*0.01,           # bias vector\n",
        "                     rs.randn(1, outsize, dict_size)*0.5 + 0.0*1j, # KAF coefficients (real part)\n",
        "                     rs.randn(1, outsize, dict_size)*0.5 + 0.0*1j) # KAF coefficients (imaginary part)\n",
        "                     for insize, outsize in zip(l[:-1], l[1:])]\n",
        "    \n",
        "    # Remove the KAF from the last layer (softmax)\n",
        "    w[-1] = (w[-1][0], w[-1][1])\n",
        "    \n",
        "    return w, (D, gamma)\n",
        "        \n",
        "\n",
        "\n",
        "def predict_split_kaf_nn(w, X, info):\n",
        "    \"\"\"\n",
        "    Compute the outputs of a split KAF feedforward network.\n",
        "    \"\"\"\n",
        "    \n",
        "    (D, gamma) = info\n",
        "\n",
        "    for info_layer in w:\n",
        "        outputs = np.dot(X, info_layer[0]) + info_layer[1]\n",
        "        \n",
        "        if len(info_layer) > 2:\n",
        "          # This is not the last layer\n",
        "          K_real = gauss_kernel(np.real(outputs) + 0.0*1j, D, gamma)\n",
        "          K_imaginary = gauss_kernel(np.imag(outputs) + 0.0*1j, D, gamma)\n",
        "          X = np.sum(K_real*np.real(info_layer[2]), axis=2) + 1j*np.sum(K_imaginary*np.real(info_layer[3]), axis=2)\n",
        "    \n",
        "    return softmax(np.real(outputs)**2 + np.imag(outputs)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FvUvNsTnENo4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step  4 - Training"
      ]
    },
    {
      "metadata": {
        "id": "doknrorkE9xp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Utility class for iterating over the training data\n",
        "class MiniBatchIterator():\n",
        "    \n",
        "    def __init__(self, X, y, B, random_seed=1):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.B = B\n",
        "        self.idx = 0\n",
        "        self.indices = np.arange(self.X.shape[0])\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    # Create new mini-batch\n",
        "    def next_minibatch(self, shuffle=True):\n",
        "        \n",
        "        if self.B == np.inf:\n",
        "            return (self.X, self.y)\n",
        "        \n",
        "        # Shuffle the data if we are starting a new epoch\n",
        "        if self.idx == 0 and shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "            \n",
        "        if self.idx + self.B >= self.X.shape[0]:\n",
        "            end_idx = self.X.shape[0]\n",
        "            reset = True\n",
        "        else:\n",
        "            end_idx = self.idx + self.B\n",
        "            reset = False\n",
        "        \n",
        "        X_batch = self.X[self.indices[self.idx:end_idx]]\n",
        "        y_batch = self.y[self.indices[self.idx:end_idx]]\n",
        "        \n",
        "        if reset:\n",
        "            self.idx = 0\n",
        "        else:\n",
        "            self.idx += self.B\n",
        "        \n",
        "        return (X_batch, y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FaffE8J5Eepz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize network\n",
        "w, info = init_split_kaf_nn(28 * 28, 10, [100, 100])\n",
        "\n",
        "# Flatten the parameters\n",
        "from autograd.misc.flatten import flatten\n",
        "w_flat, unflattener = flatten(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rX9E3oJiEMtg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Define the cost function\n",
        "def cross_entropy_loss(w_flat, x_batch, y_batch):\n",
        "    \"\"\"\n",
        "    Mean cross-entropy over the batch of data.\n",
        "    \"\"\"\n",
        "    pred = predict_split_kaf_nn(unflattener(w_flat), x_batch, info)\n",
        "    return np.mean(-np.sum(y_batch*np.log(pred), axis=1))\n",
        "\n",
        "# Define a function to compute the accuracy\n",
        "def accuracy_score(y, y_pred):\n",
        "    return np.mean(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zF0oV_lbFdnu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Optimization algorithm\n",
        "def adagrad(f_grad, w, training_iterator, step_size = 0.01, fudge_factor = 1e-6, max_it=5000):\n",
        "    # Adapted from:\n",
        "    # https://github.com/benbo/adagrad\n",
        "\n",
        "    gti = np.zeros(len(w)) + 1j*np.zeros(len(w))\n",
        "    err = np.zeros(max_it)\n",
        "    grad = np.zeros(max_it)\n",
        "\n",
        "    for t in range(max_it):\n",
        "        \n",
        "        (X_batch, y_batch) = training_iterator.next_minibatch()\n",
        "    \n",
        "        err[t], g = f_grad(w, X_batch, y_batch)\n",
        "        g = np.conj(g)\n",
        "        grad[t] = np.sum(np.absolute(g)**2)  \n",
        "        \n",
        "        gti+=g**2\n",
        "        adjusted_grad = g / (fudge_factor + np.sqrt(gti))\n",
        "        w = w - step_size*adjusted_grad\n",
        "        \n",
        "        if t % 1000 == 0:\n",
        "          print('Iteration {} ({:.2f} %), current loss is {}...'.format(t, t / max_it * 100, err[t]))\n",
        "          \n",
        "    return w, err, grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLpKqfZoFwcN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Main training part\n",
        "from autograd import value_and_grad\n",
        "\n",
        "# Set seed for PRNG\n",
        "np.random.seed(0)\n",
        "\n",
        "print('Training KAF networks...')\n",
        "\n",
        "# Define the training iterator\n",
        "batch_iterator = MiniBatchIterator(X_train, y_train, 40, random_seed=0)\n",
        "        \n",
        "# Define gradient\n",
        "f_grad = value_and_grad(cross_entropy_loss)\n",
        "            \n",
        "# Train\n",
        "w_final, loss_history, grad_history = adagrad(f_grad, w_flat, batch_iterator)\n",
        "            \n",
        "# Evaluate (needs a lot more iterations!)\n",
        "print('Computing test error...')\n",
        "y_pred = predict_split_kaf_nn(unflattener(w_final), X_test, info)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Final error on test set: ', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}